# Configuration for Qwen3-0.6B training with GSM8K
# Optimized for single GPU training with Atropos integration

# Self-contained configuration for demo
defaults:
  - _self_

# Model configuration optimized for Qwen3-0.6B
actor_rollout_ref:
  hybrid_engine: true  # Required by VeRL
  model:
    path: "Qwen/Qwen3-0.6B"
    use_shm: false
    lora_rank: 0  # Full fine-tuning for small model
    lora_alpha: 32
    use_remove_padding: true
    enable_gradient_checkpointing: false  # Not needed for small model
    trust_remote_code: true  # Required for Qwen models

  # Actor training configuration optimized for 0.6B
  actor:
    strategy: "fsdp"
    use_dynamic_bsz: false  # Use fixed batch sizes
    optim:
      lr: 5e-6  # Higher learning rate for smaller model
    ppo_mini_batch_size: 32   # Smaller for faster demo
    ppo_micro_batch_size: null  # Use per_gpu version instead
    ppo_micro_batch_size_per_gpu: 8
    ppo_epochs: 1
    
    # GRPO-specific settings
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: "low_var_kl"
    entropy_coeff: 0
    clip_ratio: 0.2
    loss_agg_mode: "token-mean"
    
    # FSDP configuration for single GPU
    fsdp_config:
      param_offload: false
      optimizer_offload: false

  # Rollout configuration optimized for 0.6B
  rollout:
    name: "vllm"
    log_prob_micro_batch_size: null  # Use per_gpu version instead
    log_prob_micro_batch_size_per_gpu: 8
    tensor_model_parallel_size: 1  # Single GPU
    gpu_memory_utilization: 0.8  # Higher utilization for small model
    max_model_len: 1024  # Shorter for demo
    enforce_eager: false
    disable_log_stats: false
    enable_prefix_caching: true
    kv_cache_dtype: "auto"
    n: 2  # Fewer samples for faster demo
    load_format: "safetensors"
    layered_summon: true
    mode: "sync"
    temperature: 0.7  # Required for validation
    multi_turn:
      enable: false  # Disable multi-turn for demo
    val_kwargs:
      do_sample: false  # Deterministic validation
      temperature: 0.7
      n: 1  # Number of validation samples per prompt

  # Reference model configuration
  ref:
    log_prob_micro_batch_size: null  # Use per_gpu version instead
    log_prob_micro_batch_size_per_gpu: 32
    fsdp_config:
      param_offload: false  # Keep in GPU memory for small model

# Data configuration optimized for GSM8K
data:
  train_files: null  # Provided by Atropos
  val_files: null
  train_batch_size: 64   # Smaller for faster demo
  max_prompt_length: 512  # Sufficient for GSM8K problems
  max_response_length: 512  # Shorter for faster demo
  filter_overlong_prompts: true
  truncation: "error"
  shuffle: false
  trust_remote_code: true
  reward_fn_key: null  # Atropos provides rewards directly
  val_batch_size: null  # Use full validation set

# Trainer configuration for single GPU
trainer:
  strategy: "fsdp"
  critic_warmup: 0
  logger: ["console", "wandb"]
  project_name: "qwen3_gsm8k"
  experiment_name: "qwen3_0.6b_gsm8k_training"
  wandb_group: "qwen3_gsm8k"
  n_gpus_per_node: 1  # Single GPU
  nnodes: 1
  save_freq: 2   # Save every 2 epochs for demo
  test_freq: 2    # Test every 2 epochs
  total_epochs: 10  # Demo with 10 epochs
  total_training_steps: null  # Will be calculated from epochs
  output_dir: "/tmp/qwen3_gsm8k_checkpoints"
  device: "cuda"  # GPU device

# Critic configuration (minimal for GRPO)
critic:
  strategy: "fsdp"
  use_dynamic_bsz: false  # Use fixed batch sizes
  optim:
    lr: 1e-5
  train_micro_batch_size_per_gpu: 32
  ppo_micro_batch_size: null  # Use per_gpu version instead
  ppo_micro_batch_size_per_gpu: 32
  ppo_mini_batch_size: 32  # Mini batch size for critic
  fsdp_config:
    param_offload: false
    optimizer_offload: false

# Atropos configuration
atropos:
  api_url: "http://localhost:8000"
  timeout: 60  # Longer timeout for GSM8K processing
  batch_retry_attempts: 10
  batch_retry_delay: 0.5
  batch_max_wait_time: 30.0
  environment_sync_timeout: 120
  max_environments: 5

# Algorithm configuration
algorithm:
  adv_estimator: "grpo_atropos"
  use_kl_in_reward: false

# Reward model (disabled for Atropos)
reward_model:
  enable: false

# Ray configuration for single GPU
ray_init:
  num_cpus: 8  # Adjust based on your instance
  num_gpus: 1  # Ensure Ray detects the GPU 