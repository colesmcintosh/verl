# Configuration for Atropos SFT Trainer with token-level advantage weighting
# Based on VERL's standard SFT config with Atropos-specific additions

# Model configuration
model:
  # Path to the pretrained model (local or HuggingFace)
  partial_pretrain: "Qwen/Qwen2.5-1.5B-Instruct"  # Example model
  strategy: "fsdp2"  # "fsdp" or "fsdp2"
  trust_remote_code: false
  external_lib: null  # Optional external library import

# Data configuration
data:
  # Standard data config
  train_batch_size: 32
  micro_batch_size_per_gpu: 2
  max_length: 2048
  balance_dp_token: true
  
  # Atropos-specific data config
  atropos:
    # API endpoint for live data fetching (optional)
    # api_url: "http://localhost:8000"
    
    # Static data file for testing (optional)
    data_path: "path/to/atropos_data.jsonl"
    
    # Data fetching parameters
    batch_size: 32
    refresh_interval: 10  # seconds
    max_retries: 3
    timeout: 30
    
    # Whether to use training or validation split
    data_type: "training"

# Atropos advantage weighting configuration
use_advantage_weighting: true
advantage_normalization: "batch"  # "none", "batch", "global"
advantage_clipping: [-5.0, 5.0]  # [min, max] or null for no clipping

# Sequence parallelism configuration
ulysses_sequence_parallel_size: 1
use_remove_padding: false

# Optimizer configuration
optim:
  lr: 1.0e-5
  weight_decay: 0.01
  lr_scheduler: "cosine"
  lr_scheduler_kwargs:
    warmup_steps: 100
    total_steps: 1000
  clip_grad: 1.0

# Trainer configuration
trainer:
  total_epochs: 3
  total_training_steps: null  # If set, overrides total_epochs
  project_name: "atropos_sft"
  experiment_name: "advantage_weighted_sft"
  logger: "wandb"  # "wandb", "tensorboard", or "none"
  
  # Checkpoint and save configuration
  default_local_dir: "./checkpoints"
  default_hdfs_dir: null  # Optional HDFS path for checkpoint backup
  save_steps: 100
  eval_steps: 50
  
  # Validation configuration
  val_check_interval: 0.1  # Check validation every 10% of epoch
  val_batch_size: 8

# Hardware configuration
devices:
  # Number of GPUs to use
  num_gpus: 1
  
  # Mixed precision training
  mixed_precision: true
  
  # CPU offloading for FSDP
  cpu_offload: false

# Hydra configuration
defaults:
  - _self_

hydra:
  job:
    chdir: true
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S} 